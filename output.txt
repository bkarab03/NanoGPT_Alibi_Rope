---------------------------
{'out_dir': 'out-shakespeare-char', 'eval_interval': 1000, 'log_interval': 10, 'eval_iters': 2, 'eval_only': False, 'always_save_checkpoint': False, 'init_from': 'scratch', 'wandb_log': False, 'wandb_project': 'shakespeare-char', 'wandb_run_name': 'mini-gpt', 'dataset': 'shakespeare_char', 'gradient_accumulation_steps': 1, 'batch_size': 12, 'block_size': 1024, 'n_layer': 6, 'n_head': 4, 'n_embd': 1024, 'dropout': 0.0, 'bias': False, 'learning_rate': 0.001, 'max_iters': 5000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 100, 'lr_decay_iters': 2000, 'min_lr': 0.0001, 'backend': 'nccl', 'device': 'cuda', 'dtype': 'float16', 'compile': False, 'pos_enc_type': 'causal', 'model_type': 'GPT', 'attention_type': 'causal', 'max_time_minutes': 3}
attention_type causal encoding causal iter 10: loss 3.1209%
---------------------------
---------------------------
{'out_dir': 'out-shakespeare-char', 'eval_interval': 1000, 'log_interval': 10, 'eval_iters': 2, 'eval_only': False, 'always_save_checkpoint': False, 'init_from': 'scratch', 'wandb_log': False, 'wandb_project': 'shakespeare-char', 'wandb_run_name': 'mini-gpt', 'dataset': 'shakespeare_char', 'gradient_accumulation_steps': 1, 'batch_size': 12, 'block_size': 256, 'n_layer': 6, 'n_head': 4, 'n_embd': 256, 'dropout': 0.0, 'bias': False, 'learning_rate': 0.001, 'max_iters': 5000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 100, 'lr_decay_iters': 2000, 'min_lr': 0.0001, 'backend': 'nccl', 'device': 'cuda', 'dtype': 'float16', 'compile': False, 'pos_enc_type': 'orijinal', 'model_type': 'GPT', 'attention_type': 'causal', 'max_time_minutes': 3, 'disable_attn': True}
attention_type causal encoding orijinal iter 1046: loss 2.4518%
---------------------------
---------------------------
{'out_dir': 'out-shakespeare-char', 'eval_interval': 1000, 'log_interval': 10, 'eval_iters': 2, 'eval_only': False, 'always_save_checkpoint': False, 'init_from': 'scratch', 'wandb_log': False, 'wandb_project': 'shakespeare-char', 'wandb_run_name': 'mini-gpt', 'dataset': 'shakespeare_char', 'gradient_accumulation_steps': 1, 'batch_size': 12, 'block_size': 256, 'n_layer': 6, 'n_head': 4, 'n_embd': 256, 'dropout': 0.0, 'bias': False, 'learning_rate': 0.001, 'max_iters': 5000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 100, 'lr_decay_iters': 2000, 'min_lr': 0.0001, 'backend': 'nccl', 'device': 'cuda', 'dtype': 'float16', 'compile': False, 'pos_enc_type': 'orijinal', 'model_type': 'GPT', 'attention_type': 'causal', 'max_time_minutes': 3, 'disable_attn': False}
attention_type causal encoding orijinal iter 655: loss 1.9028%
---------------------------
---------------------------
{'out_dir': 'out-shakespeare-char', 'eval_interval': 1000, 'log_interval': 10, 'eval_iters': 2, 'eval_only': False, 'always_save_checkpoint': False, 'init_from': 'scratch', 'wandb_log': False, 'wandb_project': 'shakespeare-char', 'wandb_run_name': 'mini-gpt', 'dataset': 'shakespeare_char', 'gradient_accumulation_steps': 1, 'batch_size': 12, 'block_size': 256, 'n_layer': 6, 'n_head': 4, 'n_embd': 256, 'dropout': 0.0, 'bias': False, 'learning_rate': 0.001, 'max_iters': 5000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 100, 'lr_decay_iters': 2000, 'min_lr': 0.0001, 'backend': 'nccl', 'device': 'cuda', 'dtype': 'float16', 'compile': False, 'pos_enc_type': 'orijinal', 'model_type': 'GPT', 'attention_type': 'causal', 'max_time_minutes': 2, 'disable_attn': True}
attention_type causal encoding orijinal iter 705: loss 2.4559%
---------------------------
---------------------------
{'out_dir': 'out-shakespeare-char', 'eval_interval': 1000, 'log_interval': 10, 'eval_iters': 2, 'eval_only': False, 'always_save_checkpoint': False, 'init_from': 'scratch', 'wandb_log': False, 'wandb_project': 'shakespeare-char', 'wandb_run_name': 'mini-gpt', 'dataset': 'shakespeare_char', 'gradient_accumulation_steps': 1, 'batch_size': 12, 'block_size': 256, 'n_layer': 6, 'n_head': 4, 'n_embd': 256, 'dropout': 0.0, 'bias': False, 'learning_rate': 0.001, 'max_iters': 5000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 100, 'lr_decay_iters': 2000, 'min_lr': 0.0001, 'backend': 'nccl', 'device': 'cuda', 'dtype': 'float16', 'compile': False, 'pos_enc_type': 'orijinal', 'model_type': 'GPT', 'attention_type': 'causal', 'max_time_minutes': 2, 'disable_attn': False}
attention_type causal encoding orijinal iter 436: loss 2.1278%
---------------------------
---------------------------
{'out_dir': 'out-shakespeare-char', 'eval_interval': 1000, 'log_interval': 10, 'eval_iters': 2, 'eval_only': False, 'always_save_checkpoint': False, 'init_from': 'scratch', 'wandb_log': False, 'wandb_project': 'shakespeare-char', 'wandb_run_name': 'mini-gpt', 'dataset': 'shakespeare_char', 'gradient_accumulation_steps': 1, 'batch_size': 12, 'block_size': 256, 'n_layer': 6, 'n_head': 4, 'n_embd': 256, 'dropout': 0.0, 'bias': False, 'learning_rate': 0.001, 'max_iters': 5000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 100, 'lr_decay_iters': 2000, 'min_lr': 0.0001, 'backend': 'nccl', 'device': 'cuda', 'dtype': 'float16', 'compile': False, 'pos_enc_type': 'orijinal', 'model_type': 'GPT', 'attention_type': 'causal', 'max_time_minutes': 5, 'disable_attn': True}
attention_type causal encoding orijinal iter 1746: loss 2.4664%
---------------------------
---------------------------
{'out_dir': 'out-shakespeare-char', 'eval_interval': 1000, 'log_interval': 10, 'eval_iters': 2, 'eval_only': False, 'always_save_checkpoint': False, 'init_from': 'scratch', 'wandb_log': False, 'wandb_project': 'shakespeare-char', 'wandb_run_name': 'mini-gpt', 'dataset': 'shakespeare_char', 'gradient_accumulation_steps': 1, 'batch_size': 12, 'block_size': 256, 'n_layer': 6, 'n_head': 4, 'n_embd': 256, 'dropout': 0.0, 'bias': False, 'learning_rate': 0.001, 'max_iters': 5000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 100, 'lr_decay_iters': 2000, 'min_lr': 0.0001, 'backend': 'nccl', 'device': 'cuda', 'dtype': 'float16', 'compile': False, 'pos_enc_type': 'orijinal', 'model_type': 'GPT', 'attention_type': 'causal', 'max_time_minutes': 5, 'disable_attn': False}
attention_type causal encoding orijinal iter 1075: loss 1.5953%
---------------------------
---------------------------
{'out_dir': 'out-shakespeare-char', 'eval_interval': 1000, 'log_interval': 10, 'eval_iters': 2, 'eval_only': False, 'always_save_checkpoint': False, 'init_from': 'scratch', 'wandb_log': False, 'wandb_project': 'shakespeare-char', 'wandb_run_name': 'mini-gpt', 'dataset': 'shakespeare_char', 'gradient_accumulation_steps': 1, 'batch_size': 12, 'block_size': 256, 'n_layer': 6, 'n_head': 4, 'n_embd': 256, 'dropout': 0.0, 'bias': False, 'learning_rate': 0.001, 'max_iters': 5000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 100, 'lr_decay_iters': 2000, 'min_lr': 0.0001, 'backend': 'nccl', 'device': 'cuda', 'dtype': 'float16', 'compile': False, 'pos_enc_type': 'orijinal', 'model_type': 'GPT', 'attention_type': 'causal', 'max_time_minutes': 3, 'disable_attn': True}
attention_type causal encoding orijinal iter 27: loss 2.9024%
---------------------------
---------------------------
{'out_dir': 'out-shakespeare-char', 'eval_interval': 1000, 'log_interval': 10, 'eval_iters': 2, 'eval_only': False, 'always_save_checkpoint': False, 'init_from': 'scratch', 'wandb_log': False, 'wandb_project': 'shakespeare-char', 'wandb_run_name': 'mini-gpt', 'dataset': 'shakespeare_char', 'gradient_accumulation_steps': 1, 'batch_size': 12, 'block_size': 256, 'n_layer': 6, 'n_head': 4, 'n_embd': 256, 'dropout': 0.0, 'bias': False, 'learning_rate': 0.001, 'max_iters': 5000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 100, 'lr_decay_iters': 2000, 'min_lr': 0.0001, 'backend': 'nccl', 'device': 'cuda', 'dtype': 'float16', 'compile': False, 'pos_enc_type': 'orijinal', 'model_type': 'GPT', 'attention_type': 'causal', 'max_time_minutes': 3, 'disable_attn': False}
attention_type causal encoding orijinal iter 26: loss 2.9529%
---------------------------
---------------------------
{'out_dir': 'out-shakespeare-char', 'eval_interval': 1000, 'log_interval': 10, 'eval_iters': 2, 'eval_only': False, 'always_save_checkpoint': False, 'init_from': 'scratch', 'wandb_log': False, 'wandb_project': 'shakespeare-char', 'wandb_run_name': 'mini-gpt', 'dataset': 'shakespeare_char', 'gradient_accumulation_steps': 1, 'batch_size': 12, 'block_size': 256, 'n_layer': 6, 'n_head': 4, 'n_embd': 256, 'dropout': 0.0, 'bias': False, 'learning_rate': 0.001, 'max_iters': 5000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 100, 'lr_decay_iters': 2000, 'min_lr': 0.0001, 'backend': 'nccl', 'device': 'cuda', 'dtype': 'float16', 'compile': False, 'pos_enc_type': 'orijinal', 'model_type': 'GPT', 'attention_type': 'causal', 'max_time_minutes': 2, 'disable_attn': True}
attention_type causal encoding orijinal iter 1368: loss 2.4709%
---------------------------
---------------------------
{'out_dir': 'out-shakespeare-char', 'eval_interval': 1000, 'log_interval': 10, 'eval_iters': 2, 'eval_only': False, 'always_save_checkpoint': False, 'init_from': 'scratch', 'wandb_log': False, 'wandb_project': 'shakespeare-char', 'wandb_run_name': 'mini-gpt', 'dataset': 'shakespeare_char', 'gradient_accumulation_steps': 1, 'batch_size': 12, 'block_size': 256, 'n_layer': 6, 'n_head': 4, 'n_embd': 256, 'dropout': 0.0, 'bias': False, 'learning_rate': 0.001, 'max_iters': 5000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 100, 'lr_decay_iters': 2000, 'min_lr': 0.0001, 'backend': 'nccl', 'device': 'cuda', 'dtype': 'float16', 'compile': False, 'pos_enc_type': 'orijinal', 'model_type': 'GPT', 'attention_type': 'causal', 'max_time_minutes': 2, 'disable_attn': False}
attention_type causal encoding orijinal iter 616: loss 1.8836%
---------------------------
---------------------------
{'out_dir': 'out-shakespeare-char', 'eval_interval': 1000, 'log_interval': 10, 'eval_iters': 2, 'eval_only': False, 'always_save_checkpoint': False, 'init_from': 'scratch', 'wandb_log': False, 'wandb_project': 'shakespeare-char', 'wandb_run_name': 'mini-gpt', 'dataset': 'shakespeare_char', 'gradient_accumulation_steps': 1, 'batch_size': 12, 'block_size': 256, 'n_layer': 3, 'n_head': 4, 'n_embd': 256, 'dropout': 0.0, 'bias': False, 'learning_rate': 0.001, 'max_iters': 5000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 100, 'lr_decay_iters': 2000, 'min_lr': 0.0001, 'backend': 'nccl', 'device': 'cuda', 'dtype': 'float16', 'compile': False, 'pos_enc_type': 'orijinal', 'model_type': 'GPT', 'attention_type': 'causal', 'max_time_minutes': 2, 'disable_attn': True}
attention_type causal encoding orijinal iter 493: loss 2.5021%
---------------------------
---------------------------
{'out_dir': 'out-shakespeare-char', 'eval_interval': 1000, 'log_interval': 10, 'eval_iters': 2, 'eval_only': False, 'always_save_checkpoint': False, 'init_from': 'scratch', 'wandb_log': False, 'wandb_project': 'shakespeare-char', 'wandb_run_name': 'mini-gpt', 'dataset': 'shakespeare_char', 'gradient_accumulation_steps': 1, 'batch_size': 12, 'block_size': 256, 'n_layer': 3, 'n_head': 4, 'n_embd': 256, 'dropout': 0.0, 'bias': False, 'learning_rate': 0.001, 'max_iters': 5000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 100, 'lr_decay_iters': 2000, 'min_lr': 0.0001, 'backend': 'nccl', 'device': 'cuda', 'dtype': 'float16', 'compile': False, 'pos_enc_type': 'orijinal', 'model_type': 'GPT', 'attention_type': 'causal', 'max_time_minutes': 2, 'disable_attn': False}
attention_type causal encoding orijinal iter 414: loss 2.2395%
---------------------------
---------------------------
{'out_dir': 'out-shakespeare-char', 'eval_interval': 1000, 'log_interval': 10, 'eval_iters': 2, 'eval_only': False, 'always_save_checkpoint': False, 'init_from': 'scratch', 'wandb_log': False, 'wandb_project': 'shakespeare-char', 'wandb_run_name': 'mini-gpt', 'dataset': 'shakespeare_char', 'gradient_accumulation_steps': 1, 'batch_size': 12, 'block_size': 256, 'n_layer': 3, 'n_head': 4, 'n_embd': 256, 'dropout': 0.0, 'bias': False, 'learning_rate': 0.001, 'max_iters': 5000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 100, 'lr_decay_iters': 2000, 'min_lr': 0.0001, 'backend': 'nccl', 'device': 'cuda', 'dtype': 'float16', 'compile': False, 'pos_enc_type': 'orijinal', 'model_type': 'GPT', 'attention_type': 'causal', 'max_time_minutes': 1, 'disable_attn': False, 'hidden_multiplier': 4, 'n_hddn_layers': 0}
attention_type causal encoding orijinal iter 432: loss 2.2147%
---------------------------
---------------------------
{'out_dir': 'out-shakespeare-char', 'eval_interval': 1000, 'log_interval': 10, 'eval_iters': 2, 'eval_only': False, 'always_save_checkpoint': False, 'init_from': 'scratch', 'wandb_log': False, 'wandb_project': 'shakespeare-char', 'wandb_run_name': 'mini-gpt', 'dataset': 'shakespeare_char', 'gradient_accumulation_steps': 1, 'batch_size': 12, 'block_size': 256, 'n_layer': 3, 'n_head': 4, 'n_embd': 256, 'dropout': 0.0, 'bias': False, 'learning_rate': 0.001, 'max_iters': 5000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 100, 'lr_decay_iters': 2000, 'min_lr': 0.0001, 'backend': 'nccl', 'device': 'cuda', 'dtype': 'float16', 'compile': False, 'pos_enc_type': 'orijinal', 'model_type': 'GPT', 'attention_type': 'causal', 'max_time_minutes': 1, 'disable_attn': False, 'hidden_multiplier': 4, 'n_hddn_layers': 1}
attention_type causal encoding orijinal iter 423: loss 2.1542%
---------------------------
---------------------------
{'out_dir': 'out-shakespeare-char', 'eval_interval': 1000, 'log_interval': 10, 'eval_iters': 2, 'eval_only': False, 'always_save_checkpoint': False, 'init_from': 'scratch', 'wandb_log': False, 'wandb_project': 'shakespeare-char', 'wandb_run_name': 'mini-gpt', 'dataset': 'shakespeare_char', 'gradient_accumulation_steps': 1, 'batch_size': 12, 'block_size': 256, 'n_layer': 3, 'n_head': 4, 'n_embd': 256, 'dropout': 0.0, 'bias': False, 'learning_rate': 0.001, 'max_iters': 5000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 100, 'lr_decay_iters': 2000, 'min_lr': 0.0001, 'backend': 'nccl', 'device': 'cuda', 'dtype': 'float16', 'compile': False, 'pos_enc_type': 'orijinal', 'model_type': 'GPT', 'attention_type': 'causal', 'max_time_minutes': 1, 'disable_attn': False, 'hidden_multiplier': 4, 'n_hddn_layers': 2}
attention_type causal encoding orijinal iter 203: loss 2.4449%
---------------------------
---------------------------
{'out_dir': 'out-shakespeare-char', 'eval_interval': 1000, 'log_interval': 10, 'eval_iters': 2, 'eval_only': False, 'always_save_checkpoint': False, 'init_from': 'scratch', 'wandb_log': False, 'wandb_project': 'shakespeare-char', 'wandb_run_name': 'mini-gpt', 'dataset': 'shakespeare_char', 'gradient_accumulation_steps': 1, 'batch_size': 12, 'block_size': 256, 'n_layer': 3, 'n_head': 4, 'n_embd': 256, 'dropout': 0.0, 'bias': False, 'learning_rate': 0.001, 'max_iters': 5000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 100, 'lr_decay_iters': 2000, 'min_lr': 0.0001, 'backend': 'nccl', 'device': 'cuda', 'dtype': 'float16', 'compile': False, 'pos_enc_type': 'orijinal', 'model_type': 'GPT', 'attention_type': 'causal', 'max_time_minutes': 1, 'disable_attn': False, 'hidden_multiplier': 4, 'n_hddn_layers': 0}
attention_type causal encoding orijinal iter 438: loss 2.2122%
---------------------------
---------------------------
{'out_dir': 'out-shakespeare-char', 'eval_interval': 1000, 'log_interval': 10, 'eval_iters': 2, 'eval_only': False, 'always_save_checkpoint': False, 'init_from': 'scratch', 'wandb_log': False, 'wandb_project': 'shakespeare-char', 'wandb_run_name': 'mini-gpt', 'dataset': 'shakespeare_char', 'gradient_accumulation_steps': 1, 'batch_size': 12, 'block_size': 256, 'n_layer': 3, 'n_head': 4, 'n_embd': 256, 'dropout': 0.0, 'bias': False, 'learning_rate': 0.001, 'max_iters': 5000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 100, 'lr_decay_iters': 2000, 'min_lr': 0.0001, 'backend': 'nccl', 'device': 'cuda', 'dtype': 'float16', 'compile': False, 'pos_enc_type': 'orijinal', 'model_type': 'GPT', 'attention_type': 'causal', 'max_time_minutes': 1, 'disable_attn': False, 'hidden_multiplier': 4, 'n_hddn_layers': 1}
attention_type causal encoding orijinal iter 431: loss 2.2058%
---------------------------
---------------------------
{'out_dir': 'out-shakespeare-char', 'eval_interval': 1000, 'log_interval': 10, 'eval_iters': 2, 'eval_only': False, 'always_save_checkpoint': False, 'init_from': 'scratch', 'wandb_log': False, 'wandb_project': 'shakespeare-char', 'wandb_run_name': 'mini-gpt', 'dataset': 'shakespeare_char', 'gradient_accumulation_steps': 1, 'batch_size': 12, 'block_size': 256, 'n_layer': 3, 'n_head': 4, 'n_embd': 256, 'dropout': 0.0, 'bias': False, 'learning_rate': 0.001, 'max_iters': 5000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 100, 'lr_decay_iters': 2000, 'min_lr': 0.0001, 'backend': 'nccl', 'device': 'cuda', 'dtype': 'float16', 'compile': False, 'pos_enc_type': 'orijinal', 'model_type': 'GPT', 'attention_type': 'causal', 'max_time_minutes': 1, 'disable_attn': False, 'hidden_multiplier': 4, 'n_hddn_layers': 2}
attention_type causal encoding orijinal iter 204: loss 2.4604%
---------------------------
---------------------------
{'out_dir': 'out-shakespeare-char', 'eval_interval': 1000, 'log_interval': 10, 'eval_iters': 2, 'eval_only': False, 'always_save_checkpoint': False, 'init_from': 'scratch', 'wandb_log': False, 'wandb_project': 'shakespeare-char', 'wandb_run_name': 'mini-gpt', 'dataset': 'shakespeare_char', 'gradient_accumulation_steps': 1, 'batch_size': 12, 'block_size': 256, 'n_layer': 3, 'n_head': 4, 'n_embd': 256, 'dropout': 0.0, 'bias': False, 'learning_rate': 0.001, 'max_iters': 5000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 100, 'lr_decay_iters': 2000, 'min_lr': 0.0001, 'backend': 'nccl', 'device': 'cuda', 'dtype': 'float16', 'compile': False, 'pos_enc_type': 'orijinal', 'model_type': 'GPT', 'attention_type': 'causal', 'max_time_minutes': 1, 'disable_attn': False, 'hidden_multiplier': 4, 'n_hddn_layers': 0}
attention_type causal encoding orijinal iter 437: loss 2.1069%
---------------------------
---------------------------
{'out_dir': 'out-shakespeare-char', 'eval_interval': 1000, 'log_interval': 10, 'eval_iters': 2, 'eval_only': False, 'always_save_checkpoint': False, 'init_from': 'scratch', 'wandb_log': False, 'wandb_project': 'shakespeare-char', 'wandb_run_name': 'mini-gpt', 'dataset': 'shakespeare_char', 'gradient_accumulation_steps': 1, 'batch_size': 12, 'block_size': 256, 'n_layer': 3, 'n_head': 4, 'n_embd': 256, 'dropout': 0.0, 'bias': False, 'learning_rate': 0.001, 'max_iters': 5000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 100, 'lr_decay_iters': 2000, 'min_lr': 0.0001, 'backend': 'nccl', 'device': 'cuda', 'dtype': 'float16', 'compile': False, 'pos_enc_type': 'orijinal', 'model_type': 'GPT', 'attention_type': 'causal', 'max_time_minutes': 1, 'disable_attn': False, 'hidden_multiplier': 4, 'n_hddn_layers': 1}
attention_type causal encoding orijinal iter 422: loss 2.1820%
---------------------------
---------------------------
{'out_dir': 'out-shakespeare-char', 'eval_interval': 1000, 'log_interval': 10, 'eval_iters': 2, 'eval_only': False, 'always_save_checkpoint': False, 'init_from': 'scratch', 'wandb_log': False, 'wandb_project': 'shakespeare-char', 'wandb_run_name': 'mini-gpt', 'dataset': 'shakespeare_char', 'gradient_accumulation_steps': 1, 'batch_size': 12, 'block_size': 256, 'n_layer': 3, 'n_head': 4, 'n_embd': 256, 'dropout': 0.0, 'bias': False, 'learning_rate': 0.001, 'max_iters': 5000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 100, 'lr_decay_iters': 2000, 'min_lr': 0.0001, 'backend': 'nccl', 'device': 'cuda', 'dtype': 'float16', 'compile': False, 'pos_enc_type': 'orijinal', 'model_type': 'GPT', 'attention_type': 'causal', 'max_time_minutes': 1, 'disable_attn': False, 'hidden_multiplier': 4, 'n_hddn_layers': 2}
attention_type causal encoding orijinal iter 204: loss 2.4604%
---------------------------
---------------------------
{'out_dir': 'out-shakespeare-char', 'eval_interval': 1000, 'log_interval': 10, 'eval_iters': 2, 'eval_only': False, 'always_save_checkpoint': False, 'init_from': 'scratch', 'wandb_log': False, 'wandb_project': 'shakespeare-char', 'wandb_run_name': 'mini-gpt', 'dataset': 'shakespeare_char', 'gradient_accumulation_steps': 1, 'batch_size': 12, 'block_size': 256, 'n_layer': 3, 'n_head': 4, 'n_embd': 256, 'dropout': 0.0, 'bias': False, 'learning_rate': 0.001, 'max_iters': 5000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 100, 'lr_decay_iters': 2000, 'min_lr': 0.0001, 'backend': 'nccl', 'device': 'cuda', 'dtype': 'float16', 'compile': False, 'pos_enc_type': 'orijinal', 'model_type': 'GPT', 'attention_type': 'causal', 'max_time_minutes': 1, 'disable_attn': False, 'hidden_multiplier': 4, 'n_hddn_layers': 0}
attention_type causal encoding orijinal iter 438: loss 2.2047%
---------------------------
---------------------------
{'out_dir': 'out-shakespeare-char', 'eval_interval': 1000, 'log_interval': 10, 'eval_iters': 2, 'eval_only': False, 'always_save_checkpoint': False, 'init_from': 'scratch', 'wandb_log': False, 'wandb_project': 'shakespeare-char', 'wandb_run_name': 'mini-gpt', 'dataset': 'shakespeare_char', 'gradient_accumulation_steps': 1, 'batch_size': 12, 'block_size': 256, 'n_layer': 3, 'n_head': 4, 'n_embd': 256, 'dropout': 0.0, 'bias': False, 'learning_rate': 0.001, 'max_iters': 5000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 100, 'lr_decay_iters': 2000, 'min_lr': 0.0001, 'backend': 'nccl', 'device': 'cuda', 'dtype': 'float16', 'compile': False, 'pos_enc_type': 'orijinal', 'model_type': 'GPT', 'attention_type': 'causal', 'max_time_minutes': 1, 'disable_attn': False, 'hidden_multiplier': 4, 'n_hddn_layers': 1}
attention_type causal encoding orijinal iter 423: loss 2.1390%
---------------------------
---------------------------
{'out_dir': 'out-shakespeare-char', 'eval_interval': 1000, 'log_interval': 10, 'eval_iters': 2, 'eval_only': False, 'always_save_checkpoint': False, 'init_from': 'scratch', 'wandb_log': False, 'wandb_project': 'shakespeare-char', 'wandb_run_name': 'mini-gpt', 'dataset': 'shakespeare_char', 'gradient_accumulation_steps': 1, 'batch_size': 12, 'block_size': 256, 'n_layer': 3, 'n_head': 4, 'n_embd': 256, 'dropout': 0.0, 'bias': False, 'learning_rate': 0.001, 'max_iters': 5000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 100, 'lr_decay_iters': 2000, 'min_lr': 0.0001, 'backend': 'nccl', 'device': 'cuda', 'dtype': 'float16', 'compile': False, 'pos_enc_type': 'orijinal', 'model_type': 'GPT', 'attention_type': 'causal', 'max_time_minutes': 1, 'disable_attn': False, 'hidden_multiplier': 4, 'n_hddn_layers': 2}
attention_type causal encoding orijinal iter 204: loss 2.4605%
---------------------------
---------------------------
{'out_dir': 'out-shakespeare-char', 'eval_interval': 1000, 'log_interval': 10, 'eval_iters': 2, 'eval_only': False, 'always_save_checkpoint': False, 'init_from': 'scratch', 'wandb_log': False, 'wandb_project': 'shakespeare-char', 'wandb_run_name': 'mini-gpt', 'dataset': 'shakespeare_char', 'gradient_accumulation_steps': 1, 'batch_size': 12, 'block_size': 256, 'n_layer': 3, 'n_head': 4, 'n_embd': 256, 'dropout': 0.0, 'bias': False, 'learning_rate': 0.001, 'max_iters': 5000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 100, 'lr_decay_iters': 2000, 'min_lr': 0.0001, 'backend': 'nccl', 'device': 'cuda', 'dtype': 'float16', 'compile': False, 'pos_enc_type': 'orijinal', 'model_type': 'GPT', 'attention_type': 'causal', 'max_time_minutes': 1, 'disable_attn': False, 'hidden_multiplier': 4, 'n_hddn_layers': 0}
attention_type causal encoding orijinal iter 434: loss 2.1736%
---------------------------
---------------------------
{'out_dir': 'out-shakespeare-char', 'eval_interval': 1000, 'log_interval': 10, 'eval_iters': 2, 'eval_only': False, 'always_save_checkpoint': False, 'init_from': 'scratch', 'wandb_log': False, 'wandb_project': 'shakespeare-char', 'wandb_run_name': 'mini-gpt', 'dataset': 'shakespeare_char', 'gradient_accumulation_steps': 1, 'batch_size': 12, 'block_size': 256, 'n_layer': 3, 'n_head': 4, 'n_embd': 256, 'dropout': 0.0, 'bias': False, 'learning_rate': 0.001, 'max_iters': 5000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 100, 'lr_decay_iters': 2000, 'min_lr': 0.0001, 'backend': 'nccl', 'device': 'cuda', 'dtype': 'float16', 'compile': False, 'pos_enc_type': 'orijinal', 'model_type': 'GPT', 'attention_type': 'causal', 'max_time_minutes': 1, 'disable_attn': False, 'hidden_multiplier': 4, 'n_hddn_layers': 4}
attention_type causal encoding orijinal iter 99: loss 2.5494%
---------------------------
---------------------------
{'out_dir': 'out-shakespeare-char', 'eval_interval': 1000, 'log_interval': 10, 'eval_iters': 2, 'eval_only': False, 'always_save_checkpoint': False, 'init_from': 'scratch', 'wandb_log': False, 'wandb_project': 'shakespeare-char', 'wandb_run_name': 'mini-gpt', 'dataset': 'shakespeare_char', 'gradient_accumulation_steps': 1, 'batch_size': 12, 'block_size': 256, 'n_layer': 3, 'n_head': 4, 'n_embd': 256, 'dropout': 0.0, 'bias': False, 'learning_rate': 0.001, 'max_iters': 5000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 100, 'lr_decay_iters': 2000, 'min_lr': 0.0001, 'backend': 'nccl', 'device': 'cuda', 'dtype': 'float16', 'compile': False, 'pos_enc_type': 'orijinal', 'model_type': 'GPT', 'attention_type': 'causal', 'max_time_minutes': 1, 'disable_attn': False, 'hidden_multiplier': 4, 'n_hddn_layers': 8}
attention_type causal encoding orijinal iter 48: loss 3.1540%
---------------------------
---------------------------
{'out_dir': 'out-shakespeare-char', 'eval_interval': 1000, 'log_interval': 10, 'eval_iters': 2, 'eval_only': False, 'always_save_checkpoint': False, 'init_from': 'scratch', 'wandb_log': False, 'wandb_project': 'shakespeare-char', 'wandb_run_name': 'mini-gpt', 'dataset': 'shakespeare_char', 'gradient_accumulation_steps': 1, 'batch_size': 12, 'block_size': 256, 'n_layer': 3, 'n_head': 4, 'n_embd': 256, 'dropout': 0.0, 'bias': False, 'learning_rate': 0.001, 'max_iters': 5000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 100, 'lr_decay_iters': 2000, 'min_lr': 0.0001, 'backend': 'nccl', 'device': 'cuda', 'dtype': 'float16', 'compile': False, 'pos_enc_type': 'orijinal', 'model_type': 'GPT', 'attention_type': 'causal', 'max_time_minutes': 1, 'disable_attn': False, 'hidden_multiplier': 4, 'n_hddn_layers': 0}
attention_type causal encoding orijinal iter 432: loss 2.2164%
---------------------------
---------------------------
{'out_dir': 'out-shakespeare-char', 'eval_interval': 1000, 'log_interval': 10, 'eval_iters': 2, 'eval_only': False, 'always_save_checkpoint': False, 'init_from': 'scratch', 'wandb_log': False, 'wandb_project': 'shakespeare-char', 'wandb_run_name': 'mini-gpt', 'dataset': 'shakespeare_char', 'gradient_accumulation_steps': 1, 'batch_size': 12, 'block_size': 256, 'n_layer': 3, 'n_head': 4, 'n_embd': 256, 'dropout': 0.0, 'bias': False, 'learning_rate': 0.001, 'max_iters': 5000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 100, 'lr_decay_iters': 2000, 'min_lr': 0.0001, 'backend': 'nccl', 'device': 'cuda', 'dtype': 'float16', 'compile': False, 'pos_enc_type': 'orijinal', 'model_type': 'GPT', 'attention_type': 'causal', 'max_time_minutes': 1, 'disable_attn': False, 'hidden_multiplier': 4, 'n_hddn_layers': 4}
attention_type causal encoding orijinal iter 98: loss 2.5221%
---------------------------
---------------------------
{'out_dir': 'out-shakespeare-char', 'eval_interval': 1000, 'log_interval': 10, 'eval_iters': 2, 'eval_only': False, 'always_save_checkpoint': False, 'init_from': 'scratch', 'wandb_log': False, 'wandb_project': 'shakespeare-char', 'wandb_run_name': 'mini-gpt', 'dataset': 'shakespeare_char', 'gradient_accumulation_steps': 1, 'batch_size': 12, 'block_size': 256, 'n_layer': 3, 'n_head': 4, 'n_embd': 256, 'dropout': 0.0, 'bias': False, 'learning_rate': 0.001, 'max_iters': 5000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 100, 'lr_decay_iters': 2000, 'min_lr': 0.0001, 'backend': 'nccl', 'device': 'cuda', 'dtype': 'float16', 'compile': False, 'pos_enc_type': 'orijinal', 'model_type': 'GPT', 'attention_type': 'causal', 'max_time_minutes': 1, 'disable_attn': False, 'hidden_multiplier': 4, 'n_hddn_layers': 8}
attention_type causal encoding orijinal iter 48: loss 3.1529%
---------------------------
---------------------------
{'out_dir': 'out-shakespeare-char', 'eval_interval': 1000, 'log_interval': 10, 'eval_iters': 2, 'eval_only': False, 'always_save_checkpoint': False, 'init_from': 'scratch', 'wandb_log': False, 'wandb_project': 'shakespeare-char', 'wandb_run_name': 'mini-gpt', 'dataset': 'shakespeare_char', 'gradient_accumulation_steps': 1, 'batch_size': 12, 'block_size': 256, 'n_layer': 3, 'n_head': 4, 'n_embd': 256, 'dropout': 0.0, 'bias': False, 'learning_rate': 0.001, 'max_iters': 5000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 100, 'lr_decay_iters': 2000, 'min_lr': 0.0001, 'backend': 'nccl', 'device': 'cuda', 'dtype': 'float16', 'compile': False, 'pos_enc_type': 'orijinal', 'model_type': 'GPT', 'attention_type': 'causal', 'max_time_minutes': 2, 'disable_attn': False, 'hidden_multiplier': 4, 'n_hddn_layers': 0}
attention_type causal encoding orijinal iter 875: loss 1.6669%
---------------------------
---------------------------
{'out_dir': 'out-shakespeare-char', 'eval_interval': 1000, 'log_interval': 10, 'eval_iters': 2, 'eval_only': False, 'always_save_checkpoint': False, 'init_from': 'scratch', 'wandb_log': False, 'wandb_project': 'shakespeare-char', 'wandb_run_name': 'mini-gpt', 'dataset': 'shakespeare_char', 'gradient_accumulation_steps': 1, 'batch_size': 12, 'block_size': 256, 'n_layer': 3, 'n_head': 4, 'n_embd': 256, 'dropout': 0.0, 'bias': False, 'learning_rate': 0.001, 'max_iters': 5000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 100, 'lr_decay_iters': 2000, 'min_lr': 0.0001, 'backend': 'nccl', 'device': 'cuda', 'dtype': 'float16', 'compile': False, 'pos_enc_type': 'orijinal', 'model_type': 'GPT', 'attention_type': 'causal', 'max_time_minutes': 2, 'disable_attn': False, 'hidden_multiplier': 4, 'n_hddn_layers': 0}
attention_type causal encoding orijinal iter 873: loss 3.2936%
---------------------------
---------------------------
{'out_dir': 'out-shakespeare-char', 'eval_interval': 1000, 'log_interval': 10, 'eval_iters': 2, 'eval_only': False, 'always_save_checkpoint': False, 'init_from': 'scratch', 'wandb_log': False, 'wandb_project': 'shakespeare-char', 'wandb_run_name': 'mini-gpt', 'dataset': 'shakespeare_char', 'gradient_accumulation_steps': 1, 'batch_size': 12, 'block_size': 256, 'n_layer': 3, 'n_head': 4, 'n_embd': 256, 'dropout': 0.0, 'bias': False, 'learning_rate': 0.001, 'max_iters': 5000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 100, 'lr_decay_iters': 2000, 'min_lr': 0.0001, 'backend': 'nccl', 'device': 'cuda', 'dtype': 'float16', 'compile': False, 'pos_enc_type': 'orijinal', 'model_type': 'GPT', 'attention_type': 'causal', 'max_time_minutes': 2, 'disable_attn': False, 'hidden_multiplier': 4, 'n_hddn_layers': 0}
attention_type causal encoding orijinal iter 865: loss 1.7055%
---------------------------
---------------------------
{'out_dir': 'out-shakespeare-char', 'eval_interval': 1000, 'log_interval': 10, 'eval_iters': 2, 'eval_only': False, 'always_save_checkpoint': False, 'init_from': 'scratch', 'wandb_log': False, 'wandb_project': 'shakespeare-char', 'wandb_run_name': 'mini-gpt', 'dataset': 'shakespeare_char', 'gradient_accumulation_steps': 1, 'batch_size': 12, 'block_size': 256, 'n_layer': 3, 'n_head': 4, 'n_embd': 256, 'dropout': 0.0, 'bias': False, 'learning_rate': 0.001, 'max_iters': 5000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 100, 'lr_decay_iters': 2000, 'min_lr': 0.0001, 'backend': 'nccl', 'device': 'cuda', 'dtype': 'float16', 'compile': False, 'pos_enc_type': 'orijinal', 'model_type': 'GPT', 'attention_type': 'attention_with_memory', 'max_time_minutes': 1, 'disable_attn': False, 'hidden_multiplier': 4, 'n_hddn_layers': 0}
attention_type attention_with_memory encoding orijinal iter 441: loss 2.1601%
---------------------------
---------------------------
{'out_dir': 'out-shakespeare-char', 'eval_interval': 1000, 'log_interval': 10, 'eval_iters': 2, 'eval_only': False, 'always_save_checkpoint': False, 'init_from': 'scratch', 'wandb_log': False, 'wandb_project': 'shakespeare-char', 'wandb_run_name': 'mini-gpt', 'dataset': 'shakespeare_char', 'gradient_accumulation_steps': 1, 'batch_size': 12, 'block_size': 256, 'n_layer': 3, 'n_head': 4, 'n_embd': 256, 'dropout': 0.0, 'bias': False, 'learning_rate': 0.001, 'max_iters': 5000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 100, 'lr_decay_iters': 2000, 'min_lr': 0.0001, 'backend': 'nccl', 'device': 'cuda', 'dtype': 'float16', 'compile': False, 'pos_enc_type': 'orijinal', 'model_type': 'GPT', 'attention_type': 'causal', 'max_time_minutes': 1, 'disable_attn': False, 'hidden_multiplier': 4, 'n_hddn_layers': 0}
attention_type causal encoding orijinal iter 426: loss 2.1513%
---------------------------
---------------------------
{'out_dir': 'out-shakespeare-char', 'eval_interval': 200, 'log_interval': 10, 'eval_iters': 3, 'eval_only': False, 'always_save_checkpoint': False, 'init_from': 'scratch', 'wandb_log': False, 'wandb_project': 'shakespeare-char', 'wandb_run_name': 'mini-gpt', 'dataset': 'shakespeare_char', 'gradient_accumulation_steps': 1, 'batch_size': 12, 'block_size': 512, 'n_layer': 6, 'n_head': 4, 'n_embd': 512, 'dropout': 0.0, 'bias': False, 'learning_rate': 0.001, 'max_iters': 5000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 100, 'lr_decay_iters': 2000, 'min_lr': 0.0001, 'backend': 'nccl', 'device': 'cuda', 'dtype': 'float16', 'compile': False, 'pos_enc_type': 'orijinal', 'model_type': 'GPT', 'attention_type': 'attention_with_memory', 'max_time_minutes': 5, 'disable_attn': False, 'hidden_multiplier': 4, 'n_hddn_layers': 0}
attention_type attention_with_memory encoding orijinal iter 151: loss 2.4658%
---------------------------
---------------------------
{'out_dir': 'out-shakespeare-char', 'eval_interval': 200, 'log_interval': 10, 'eval_iters': 3, 'eval_only': False, 'always_save_checkpoint': False, 'init_from': 'scratch', 'wandb_log': False, 'wandb_project': 'shakespeare-char', 'wandb_run_name': 'mini-gpt', 'dataset': 'shakespeare_char', 'gradient_accumulation_steps': 1, 'batch_size': 12, 'block_size': 512, 'n_layer': 6, 'n_head': 4, 'n_embd': 512, 'dropout': 0.0, 'bias': False, 'learning_rate': 0.001, 'max_iters': 5000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 100, 'lr_decay_iters': 2000, 'min_lr': 0.0001, 'backend': 'nccl', 'device': 'cuda', 'dtype': 'float16', 'compile': False, 'pos_enc_type': 'orijinal', 'model_type': 'GPT', 'attention_type': 'causal', 'max_time_minutes': 5, 'disable_attn': False, 'hidden_multiplier': 4, 'n_hddn_layers': 0}
attention_type causal encoding orijinal iter 150: loss 2.4589%
---------------------------
---------------------------
{'out_dir': 'out-shakespeare-char', 'eval_interval': 150, 'log_interval': 10, 'eval_iters': 3, 'eval_only': False, 'always_save_checkpoint': False, 'init_from': 'scratch', 'wandb_log': False, 'wandb_project': 'shakespeare-char', 'wandb_run_name': 'mini-gpt', 'dataset': 'shakespeare_char', 'gradient_accumulation_steps': 1, 'batch_size': 12, 'block_size': 128, 'n_layer': 6, 'n_head': 4, 'n_embd': 128, 'dropout': 0.0, 'bias': False, 'learning_rate': 0.001, 'max_iters': 5000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 100, 'lr_decay_iters': 2000, 'min_lr': 0.0001, 'backend': 'nccl', 'device': 'cuda', 'dtype': 'float16', 'compile': False, 'pos_enc_type': 'orijinal', 'model_type': 'GPT', 'attention_type': 'attention_with_memory', 'max_time_minutes': 3, 'disable_attn': False, 'hidden_multiplier': 4, 'n_hddn_layers': 0}
attention_type attention_with_memory encoding orijinal iter 4174: loss 1.4817%
---------------------------
---------------------------
{'out_dir': 'out-shakespeare-char', 'eval_interval': 150, 'log_interval': 10, 'eval_iters': 3, 'eval_only': False, 'always_save_checkpoint': False, 'init_from': 'scratch', 'wandb_log': False, 'wandb_project': 'shakespeare-char', 'wandb_run_name': 'mini-gpt', 'dataset': 'shakespeare_char', 'gradient_accumulation_steps': 1, 'batch_size': 12, 'block_size': 128, 'n_layer': 6, 'n_head': 4, 'n_embd': 128, 'dropout': 0.0, 'bias': False, 'learning_rate': 0.001, 'max_iters': 5000, 'weight_decay': 0.1, 'beta1': 0.9, 'beta2': 0.99, 'grad_clip': 1.0, 'decay_lr': True, 'warmup_iters': 100, 'lr_decay_iters': 2000, 'min_lr': 0.0001, 'backend': 'nccl', 'device': 'cuda', 'dtype': 'float16', 'compile': False, 'pos_enc_type': 'orijinal', 'model_type': 'GPT', 'attention_type': 'causal', 'max_time_minutes': 3, 'disable_attn': False, 'hidden_multiplier': 4, 'n_hddn_layers': 0}
attention_type causal encoding orijinal iter 4086: loss 1.5335%
---------------------------
